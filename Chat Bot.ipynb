{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Practical Guide to Building a Conversational Chat Bot\n",
    "\n",
    "#### **By:** <b>Imonmion Emmanuel</b>\n",
    "\n",
    "#### For this Notebook we would using some Python Packages\n",
    "\n",
    "__Packages we would use For This Notebook:__\n",
    "1. Json\n",
    "2. Numpy\n",
    "3. NLTK\n",
    "4. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Conversational Chat Bot\n",
    "\n",
    "In the Essence of the world, it is a robot, that enables a machine to simulate human like conversations.\n",
    "To achieve this, the user interface needs to be as humanlike and conversational as possible. A conversational chatbot must understand the users intents and how to respond to the user.\n",
    "\n",
    "Fundamentally a chatbot turns raw data into conversation, The two keys bits of data that a chatbot needs to process are what people are saying to it and what it needs to respond. The easiet example to understand is a simple customer service chat Bot.\n",
    "\n",
    "* Decide the chat bot Purpose\n",
    "* Give your chat bot a persona\n",
    "* Create a conversation diagram\n",
    "* Train your chat bot and interact with it\n",
    "\n",
    "__If you intend to build a chatbot for a Customer or a Company you would need to understand the customer/company workflow, customer service and needs and Data, Beacuse Data is the key to develop a truly conversational chatbot__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Libraries to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import nltk \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening our Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chat.json\") as file:\n",
    "    bot = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through the opened Json file and appending it to different list for further iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "intent_part_x = []\n",
    "intent_part_y = []\n",
    "\n",
    "\n",
    "for intent in bot[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        token = nltk.wordpunct_tokenize(pattern) #Tokenizing the pattern\n",
    "        #Since token is a list no need of appending we just use the function extend\n",
    "        words.extend(token)\n",
    "        intent_part_x.append(token)\n",
    "        intent_part_y.append(intent[\"tag\"])\n",
    "\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the Word using NLTK Stemmer and Sorting it Alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [ps.stem(w.lower()) for w in words]\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating The Train and Output List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of word list from the labels list\n",
    "\n",
    "out_empty = [0 for i,col in enumerate(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intent_part_x is the list that contains all the available Input for the user in the Json File\n",
    "\n",
    "#Iterating through the file to create a list of bag of words to check if a user Input was present in the Json File\n",
    "\n",
    "#If present then append 1\n",
    "\n",
    "for x, doc in enumerate(intent_part_x):\n",
    "    bow = []\n",
    "\n",
    "    token = [ps.stem(w) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in token:\n",
    "            bow.append(1)\n",
    "        else:\n",
    "            bow.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(intent_part_y[x])] = 1\n",
    "\n",
    "    train.append(bow)  #Creating the Train \n",
    "    output.append(output_row) #Creating the output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting it to an numpy array since deep learning deals with array\n",
    "\n",
    "train = np.array(train)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Array\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0]\n",
      "\n",
      "Length of train array: 79\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Array\")\n",
    "\n",
    "print(train[5])\n",
    "\n",
    "\n",
    "print(f\"\\nLength of train array: {len(train[5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Deep Learning Neural Net\n",
    "\n",
    "### We would be using three layer's  \n",
    "\n",
    "#### We would be using Relu activation function and The Softmax activation since we are expected to have multiple outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would be using the Stochastic Gradient Descent and the Nesterov Accelerated Gradient Descent\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 2.6624 - accuracy: 0.1000\n",
      "Epoch 2/200\n",
      "60/60 [==============================] - 0s 933us/step - loss: 2.6849 - accuracy: 0.0500\n",
      "Epoch 3/200\n",
      "60/60 [==============================] - 0s 717us/step - loss: 2.6660 - accuracy: 0.0667\n",
      "Epoch 4/200\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.5787 - accuracy: 0.1500\n",
      "Epoch 5/200\n",
      "60/60 [==============================] - 0s 750us/step - loss: 2.6182 - accuracy: 0.1667\n",
      "Epoch 6/200\n",
      "60/60 [==============================] - 0s 950us/step - loss: 2.5947 - accuracy: 0.1667\n",
      "Epoch 7/200\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 2.5738 - accuracy: 0.1500\n",
      "Epoch 8/200\n",
      "60/60 [==============================] - 0s 767us/step - loss: 2.5364 - accuracy: 0.1167\n",
      "Epoch 9/200\n",
      "60/60 [==============================] - 0s 650us/step - loss: 2.5062 - accuracy: 0.1833\n",
      "Epoch 10/200\n",
      "60/60 [==============================] - 0s 650us/step - loss: 2.4862 - accuracy: 0.1333\n",
      "Epoch 11/200\n",
      "60/60 [==============================] - 0s 567us/step - loss: 2.5537 - accuracy: 0.1333\n",
      "Epoch 12/200\n",
      "60/60 [==============================] - 0s 617us/step - loss: 2.5084 - accuracy: 0.1167\n",
      "Epoch 13/200\n",
      "60/60 [==============================] - 0s 617us/step - loss: 2.5233 - accuracy: 0.1833\n",
      "Epoch 14/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.3212 - accuracy: 0.50 - 0s 600us/step - loss: 2.5446 - accuracy: 0.2000\n",
      "Epoch 15/200\n",
      "60/60 [==============================] - 0s 883us/step - loss: 2.3286 - accuracy: 0.2000\n",
      "Epoch 16/200\n",
      "60/60 [==============================] - 0s 867us/step - loss: 2.4373 - accuracy: 0.1833\n",
      "Epoch 17/200\n",
      "60/60 [==============================] - 0s 567us/step - loss: 2.4273 - accuracy: 0.2000\n",
      "Epoch 18/200\n",
      "60/60 [==============================] - 0s 750us/step - loss: 2.2996 - accuracy: 0.2000\n",
      "Epoch 19/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 2.3938 - accuracy: 0.1500\n",
      "Epoch 20/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 2.1640 - accuracy: 0.2833\n",
      "Epoch 21/200\n",
      "60/60 [==============================] - 0s 600us/step - loss: 2.2504 - accuracy: 0.2500\n",
      "Epoch 22/200\n",
      "60/60 [==============================] - 0s 617us/step - loss: 2.2494 - accuracy: 0.2500\n",
      "Epoch 23/200\n",
      "60/60 [==============================] - 0s 933us/step - loss: 2.3553 - accuracy: 0.2500\n",
      "Epoch 24/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 2.1481 - accuracy: 0.2333\n",
      "Epoch 25/200\n",
      "60/60 [==============================] - 0s 617us/step - loss: 2.0925 - accuracy: 0.3667\n",
      "Epoch 26/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 2.0369 - accuracy: 0.3333\n",
      "Epoch 27/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 2.0551 - accuracy: 0.3833\n",
      "Epoch 28/200\n",
      "60/60 [==============================] - 0s 600us/step - loss: 2.0891 - accuracy: 0.3333\n",
      "Epoch 29/200\n",
      "60/60 [==============================] - 0s 783us/step - loss: 2.0915 - accuracy: 0.3667\n",
      "Epoch 30/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 2.0205 - accuracy: 0.4167\n",
      "Epoch 31/200\n",
      "60/60 [==============================] - 0s 750us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "Epoch 32/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 1.9865 - accuracy: 0.3167\n",
      "Epoch 33/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 2.0546 - accuracy: 0.3667\n",
      "Epoch 34/200\n",
      "60/60 [==============================] - 0s 633us/step - loss: 1.8385 - accuracy: 0.4167\n",
      "Epoch 35/200\n",
      "60/60 [==============================] - 0s 717us/step - loss: 1.8567 - accuracy: 0.4167\n",
      "Epoch 36/200\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 1.8961 - accuracy: 0.3833\n",
      "Epoch 37/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.8715 - accuracy: 0.25 - 0s 467us/step - loss: 1.8159 - accuracy: 0.4167\n",
      "Epoch 38/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.6337 - accuracy: 0.37 - 0s 483us/step - loss: 1.7368 - accuracy: 0.4833\n",
      "Epoch 39/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.7693 - accuracy: 0.50 - 0s 467us/step - loss: 1.7144 - accuracy: 0.4667\n",
      "Epoch 40/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 1.6848 - accuracy: 0.4000\n",
      "Epoch 41/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.3076 - accuracy: 0.62 - 0s 500us/step - loss: 1.5384 - accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 1.5165 - accuracy: 0.5167\n",
      "Epoch 43/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.9769 - accuracy: 0.37 - 0s 517us/step - loss: 1.7023 - accuracy: 0.4667\n",
      "Epoch 44/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.3877 - accuracy: 0.37 - 0s 483us/step - loss: 1.6212 - accuracy: 0.4167\n",
      "Epoch 45/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 1.5829 - accuracy: 0.5167\n",
      "Epoch 46/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.4748 - accuracy: 0.50 - 0s 483us/step - loss: 1.5361 - accuracy: 0.5000\n",
      "Epoch 47/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.8603 - accuracy: 0.37 - 0s 500us/step - loss: 1.6702 - accuracy: 0.3833\n",
      "Epoch 48/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 1.5785 - accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 1.6128 - accuracy: 0.3833\n",
      "Epoch 50/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 1.4736 - accuracy: 0.4333\n",
      "Epoch 51/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 1.5371 - accuracy: 0.5000\n",
      "Epoch 52/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 1.4447 - accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 1.2738 - accuracy: 0.6000\n",
      "Epoch 54/200\n",
      "60/60 [==============================] - 0s 417us/step - loss: 1.1536 - accuracy: 0.6167\n",
      "Epoch 55/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 1.5332 - accuracy: 0.4500\n",
      "Epoch 56/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 1.3067 - accuracy: 0.4833\n",
      "Epoch 57/200\n",
      "60/60 [==============================] - 0s 567us/step - loss: 1.3542 - accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 1.3353 - accuracy: 0.5667\n",
      "Epoch 59/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 1.2248 - accuracy: 0.5500\n",
      "Epoch 60/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.2793 - accuracy: 0.37 - 0s 483us/step - loss: 1.3030 - accuracy: 0.5333\n",
      "Epoch 61/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 1.2690 - accuracy: 0.6167\n",
      "Epoch 62/200\n",
      "60/60 [==============================] - 0s 700us/step - loss: 1.5216 - accuracy: 0.4333\n",
      "Epoch 63/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 1.1133 - accuracy: 0.6500\n",
      "Epoch 64/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0935 - accuracy: 0.50 - 0s 450us/step - loss: 1.3198 - accuracy: 0.4833\n",
      "Epoch 65/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9744 - accuracy: 0.50 - 0s 500us/step - loss: 1.4083 - accuracy: 0.5500\n",
      "Epoch 66/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.3505 - accuracy: 0.62 - 0s 517us/step - loss: 1.0847 - accuracy: 0.6667\n",
      "Epoch 67/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 1.3435 - accuracy: 0.5333\n",
      "Epoch 68/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 1.0767 - accuracy: 0.6167\n",
      "Epoch 69/200\n",
      "60/60 [==============================] - 0s 400us/step - loss: 1.2432 - accuracy: 0.5667\n",
      "Epoch 70/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 1.0976 - accuracy: 0.6500\n",
      "Epoch 71/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.9287 - accuracy: 0.6833\n",
      "Epoch 72/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 1.1391 - accuracy: 0.5500\n",
      "Epoch 73/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 1.1605 - accuracy: 0.6333\n",
      "Epoch 74/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.9328 - accuracy: 0.7167\n",
      "Epoch 75/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.9962 - accuracy: 0.7000\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - ETA: 0s - loss: 1.0372 - accuracy: 0.62 - 0s 400us/step - loss: 1.1258 - accuracy: 0.5833\n",
      "Epoch 77/200\n",
      "60/60 [==============================] - 0s 600us/step - loss: 1.1892 - accuracy: 0.5333\n",
      "Epoch 78/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.9912 - accuracy: 0.6333\n",
      "Epoch 79/200\n",
      "60/60 [==============================] - 0s 400us/step - loss: 1.2042 - accuracy: 0.6500\n",
      "Epoch 80/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.87 - 0s 383us/step - loss: 1.0197 - accuracy: 0.6167\n",
      "Epoch 81/200\n",
      "60/60 [==============================] - 0s 417us/step - loss: 1.1904 - accuracy: 0.6333\n",
      "Epoch 82/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9341 - accuracy: 0.62 - 0s 433us/step - loss: 0.9616 - accuracy: 0.6500\n",
      "Epoch 83/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 1.0776 - accuracy: 0.5833\n",
      "Epoch 84/200\n",
      "60/60 [==============================] - 0s 400us/step - loss: 0.9312 - accuracy: 0.6667\n",
      "Epoch 85/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 0.8637 - accuracy: 0.7167\n",
      "Epoch 86/200\n",
      "60/60 [==============================] - 0s 383us/step - loss: 1.1860 - accuracy: 0.6500\n",
      "Epoch 87/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.4062 - accuracy: 0.62 - 0s 550us/step - loss: 0.8907 - accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.75 - 0s 433us/step - loss: 0.8706 - accuracy: 0.6667\n",
      "Epoch 89/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.8169 - accuracy: 0.7167\n",
      "Epoch 90/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9736 - accuracy: 0.37 - 0s 450us/step - loss: 0.9230 - accuracy: 0.5833\n",
      "Epoch 91/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.7490 - accuracy: 0.7833\n",
      "Epoch 92/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.8066 - accuracy: 0.7667\n",
      "Epoch 93/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.75 - 0s 500us/step - loss: 0.9526 - accuracy: 0.6833\n",
      "Epoch 94/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 1.0819 - accuracy: 0.5833\n",
      "Epoch 95/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.8436 - accuracy: 0.7000\n",
      "Epoch 96/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 1.00 - 0s 483us/step - loss: 0.8871 - accuracy: 0.7167\n",
      "Epoch 97/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.9813 - accuracy: 0.6500\n",
      "Epoch 98/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.8358 - accuracy: 0.7167\n",
      "Epoch 99/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7130 - accuracy: 0.75 - 0s 500us/step - loss: 0.8269 - accuracy: 0.7333\n",
      "Epoch 100/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.8181 - accuracy: 0.6833\n",
      "Epoch 101/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.6610 - accuracy: 0.8167\n",
      "Epoch 102/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5969 - accuracy: 0.75 - 0s 450us/step - loss: 0.5609 - accuracy: 0.8000\n",
      "Epoch 103/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.9119 - accuracy: 0.7000\n",
      "Epoch 104/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.62 - 0s 467us/step - loss: 0.7129 - accuracy: 0.7167\n",
      "Epoch 105/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.6123 - accuracy: 0.8167\n",
      "Epoch 106/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8879 - accuracy: 0.50 - 0s 467us/step - loss: 0.7235 - accuracy: 0.7500\n",
      "Epoch 107/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.4858 - accuracy: 0.8333\n",
      "Epoch 108/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.6932 - accuracy: 0.8333\n",
      "Epoch 109/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.62 - 0s 483us/step - loss: 0.6098 - accuracy: 0.8000\n",
      "Epoch 110/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.5458 - accuracy: 0.8333\n",
      "Epoch 111/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.7005 - accuracy: 0.7833\n",
      "Epoch 112/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.4455 - accuracy: 0.8667\n",
      "Epoch 113/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.5957 - accuracy: 0.8000\n",
      "Epoch 114/200\n",
      "60/60 [==============================] - 0s 817us/step - loss: 0.7506 - accuracy: 0.7333\n",
      "Epoch 115/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.7130 - accuracy: 0.8333\n",
      "Epoch 116/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.87 - 0s 600us/step - loss: 0.6657 - accuracy: 0.7833\n",
      "Epoch 117/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.6345 - accuracy: 0.7333\n",
      "Epoch 118/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.87 - 0s 500us/step - loss: 0.6716 - accuracy: 0.8500\n",
      "Epoch 119/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.5724 - accuracy: 0.7833\n",
      "Epoch 120/200\n",
      "60/60 [==============================] - 0s 650us/step - loss: 0.8480 - accuracy: 0.7833\n",
      "Epoch 121/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.8422 - accuracy: 0.6833\n",
      "Epoch 122/200\n",
      "60/60 [==============================] - 0s 667us/step - loss: 0.5228 - accuracy: 0.8167\n",
      "Epoch 123/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.5841 - accuracy: 0.8000\n",
      "Epoch 124/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.6235 - accuracy: 0.8167\n",
      "Epoch 125/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.5562 - accuracy: 0.8500\n",
      "Epoch 126/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.6831 - accuracy: 0.7667\n",
      "Epoch 127/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.7188 - accuracy: 0.7833\n",
      "Epoch 128/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 0.7161 - accuracy: 0.8167\n",
      "Epoch 129/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 0.5176 - accuracy: 0.8333\n",
      "Epoch 130/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.8248 - accuracy: 0.7500\n",
      "Epoch 131/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.87 - 0s 450us/step - loss: 0.6597 - accuracy: 0.7333\n",
      "Epoch 132/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.4799 - accuracy: 0.8333\n",
      "Epoch 133/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 0.7844 - accuracy: 0.6833\n",
      "Epoch 134/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.5647 - accuracy: 0.8167\n",
      "Epoch 135/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.7748 - accuracy: 0.7667\n",
      "Epoch 136/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 0.7031 - accuracy: 0.6833\n",
      "Epoch 137/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 0.5127 - accuracy: 0.8167\n",
      "Epoch 138/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9563 - accuracy: 0.62 - 0s 450us/step - loss: 0.6755 - accuracy: 0.7667\n",
      "Epoch 139/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.4750 - accuracy: 0.8500\n",
      "Epoch 140/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.7237 - accuracy: 0.7333\n",
      "Epoch 141/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 0.4793 - accuracy: 0.8833\n",
      "Epoch 142/200\n",
      "60/60 [==============================] - 0s 633us/step - loss: 0.5976 - accuracy: 0.8333\n",
      "Epoch 143/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.3463 - accuracy: 0.9000\n",
      "Epoch 144/200\n",
      "60/60 [==============================] - 0s 617us/step - loss: 0.3703 - accuracy: 0.8667\n",
      "Epoch 145/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.4803 - accuracy: 0.8667\n",
      "Epoch 146/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.87 - 0s 450us/step - loss: 0.4761 - accuracy: 0.8500\n",
      "Epoch 147/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 0.4380 - accuracy: 0.8333\n",
      "Epoch 148/200\n",
      "60/60 [==============================] - 0s 417us/step - loss: 0.4968 - accuracy: 0.8167\n",
      "Epoch 149/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.4132 - accuracy: 0.8833\n",
      "Epoch 150/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.5959 - accuracy: 0.7667\n",
      "Epoch 151/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.5743 - accuracy: 0.8167\n",
      "Epoch 152/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 0.6964 - accuracy: 0.8333\n",
      "Epoch 153/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.6457 - accuracy: 0.7833\n",
      "Epoch 154/200\n",
      "60/60 [==============================] - 0s 383us/step - loss: 0.6127 - accuracy: 0.7667\n",
      "Epoch 155/200\n",
      "60/60 [==============================] - 0s 400us/step - loss: 0.6966 - accuracy: 0.7500\n",
      "Epoch 156/200\n",
      "60/60 [==============================] - 0s 633us/step - loss: 0.5680 - accuracy: 0.8167\n",
      "Epoch 157/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 1.00 - 0s 467us/step - loss: 0.4240 - accuracy: 0.8500\n",
      "Epoch 158/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3672 - accuracy: 0.87 - 0s 467us/step - loss: 0.3524 - accuracy: 0.9000\n",
      "Epoch 159/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.6749 - accuracy: 0.7667\n",
      "Epoch 160/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.4092 - accuracy: 0.9000\n",
      "Epoch 161/200\n",
      "60/60 [==============================] - 0s 483us/step - loss: 0.5708 - accuracy: 0.8500\n",
      "Epoch 162/200\n",
      "60/60 [==============================] - 0s 433us/step - loss: 0.3936 - accuracy: 0.8833\n",
      "Epoch 163/200\n",
      "60/60 [==============================] - 0s 417us/step - loss: 0.4011 - accuracy: 0.8500\n",
      "Epoch 164/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.6652 - accuracy: 0.8000\n",
      "Epoch 165/200\n",
      "60/60 [==============================] - 0s 900us/step - loss: 0.4575 - accuracy: 0.8333\n",
      "Epoch 166/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.75 - 0s 817us/step - loss: 0.6596 - accuracy: 0.7333\n",
      "Epoch 167/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8145 - accuracy: 0.62 - 0s 533us/step - loss: 0.5091 - accuracy: 0.8167\n",
      "Epoch 168/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 1.00 - 0s 517us/step - loss: 0.5796 - accuracy: 0.8333\n",
      "Epoch 169/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.6669 - accuracy: 0.8000\n",
      "Epoch 170/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 0.5525 - accuracy: 0.8500\n",
      "Epoch 171/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.5730 - accuracy: 0.7667\n",
      "Epoch 172/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.4422 - accuracy: 0.8167\n",
      "Epoch 173/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.4375 - accuracy: 0.8167\n",
      "Epoch 174/200\n",
      "60/60 [==============================] - 0s 533us/step - loss: 0.6451 - accuracy: 0.7833\n",
      "Epoch 175/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 0.6784 - accuracy: 0.7833\n",
      "Epoch 176/200\n",
      "60/60 [==============================] - 0s 517us/step - loss: 0.4345 - accuracy: 0.8833\n",
      "Epoch 177/200\n",
      "60/60 [==============================] - 0s 450us/step - loss: 0.3405 - accuracy: 0.9167\n",
      "Epoch 178/200\n",
      "60/60 [==============================] - 0s 567us/step - loss: 0.5066 - accuracy: 0.8000\n",
      "Epoch 179/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.4038 - accuracy: 0.9000\n",
      "Epoch 180/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.4556 - accuracy: 0.8833\n",
      "Epoch 181/200\n",
      "60/60 [==============================] - 0s 600us/step - loss: 0.4594 - accuracy: 0.8667\n",
      "Epoch 182/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8354 - accuracy: 0.75 - 0s 483us/step - loss: 0.4868 - accuracy: 0.8333\n",
      "Epoch 183/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.87 - 0s 500us/step - loss: 0.5492 - accuracy: 0.8500\n",
      "Epoch 184/200\n",
      "60/60 [==============================] - 0s 550us/step - loss: 0.4322 - accuracy: 0.8333\n",
      "Epoch 185/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.87 - 0s 633us/step - loss: 0.2821 - accuracy: 0.9167\n",
      "Epoch 186/200\n",
      "60/60 [==============================] - 0s 467us/step - loss: 0.3398 - accuracy: 0.9000\n",
      "Epoch 187/200\n",
      "60/60 [==============================] - 0s 500us/step - loss: 0.4309 - accuracy: 0.8333\n",
      "Epoch 188/200\n",
      "60/60 [==============================] - 0s 583us/step - loss: 0.5704 - accuracy: 0.8500\n",
      "Epoch 189/200\n",
      "60/60 [==============================] - 0s 717us/step - loss: 0.6252 - accuracy: 0.8333\n",
      "Epoch 190/200\n",
      "60/60 [==============================] - 0s 983us/step - loss: 0.4135 - accuracy: 0.8667\n",
      "Epoch 191/200\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8833\n",
      "Epoch 192/200\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8833\n",
      "Epoch 193/200\n",
      "60/60 [==============================] - 0s 817us/step - loss: 0.3133 - accuracy: 0.9000\n",
      "Epoch 194/200\n",
      "60/60 [==============================] - 0s 967us/step - loss: 0.4792 - accuracy: 0.8500\n",
      "Epoch 195/200\n",
      "60/60 [==============================] - 0s 633us/step - loss: 0.5199 - accuracy: 0.8333\n",
      "Epoch 196/200\n",
      "60/60 [==============================] - 0s 817us/step - loss: 0.4301 - accuracy: 0.8833\n",
      "Epoch 197/200\n",
      "60/60 [==============================] - 0s 700us/step - loss: 0.4189 - accuracy: 0.8833\n",
      "Epoch 198/200\n",
      "60/60 [==============================] - 0s 800us/step - loss: 0.5262 - accuracy: 0.8500\n",
      "Epoch 199/200\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.8333\n",
      "Epoch 200/200\n",
      "60/60 [==============================] - 0s 883us/step - loss: 0.5092 - accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b31382da48>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Model \n",
    "model.fit(train, output, epochs=200, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Functions that would enable the user interact with the chat bot runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence,words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    This functions create a bag of word for the sentence the user enters\n",
    "    \n",
    "    Output:\n",
    "    It returns a numpy array to be used \n",
    "    Note: The Numpy array was reshaped to an array of 79 because the length of our train[0] is 79\n",
    "    If you make changes or edit the json file to suit your needs then the Length of your train will change\n",
    "    Do well to play around the codes, make changes to suit your need and understand how it works\n",
    "    \"\"\"\n",
    "    bag = [ 0 for i in range(len(words))]\n",
    "\n",
    "    sentence_token = nltk.wordpunct_tokenize(sentence)\n",
    "    sentence_token = [ps.stem(word.lower()) for word in sentence_token]\n",
    "\n",
    "    for token in sentence_token:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == token:\n",
    "                bag[i] = 1\n",
    "    \n",
    "    return np.array(bag).reshape(-1,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    This is function that Activate the ChatBot for the User to Interact with it\n",
    "    \n",
    "    Output:\n",
    "    It gives a Responses randomly.\n",
    "    We selected a threshold to prevent it from giving out irrelevatnt response to the user\n",
    "    You can quit talking with the Chat Bot by type quit in the chat Column\n",
    "    \n",
    "    Note: The Threshold was not selected Randomly, it was done after sereies of conversation with the bot to know,\n",
    "    which threshold it starts giving out irrelevant responses if the responses is not in it database\n",
    "    \"\"\"\n",
    "    print(\"Start Talking with the Bot,To Stop type quit\")\n",
    "    while True:\n",
    "        inp = input(\"You :   \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break \n",
    "\n",
    "        input_data = [bag_of_words(inp, words)]\n",
    "        results = model.predict(input_data)[0]\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        \n",
    "        #To see how the model perfoms on data that are not in the database\n",
    "        #print(f\"Model Prediction: {results[results_index]}\")\n",
    "        \n",
    "        if results[results_index] >= 0.794:\n",
    "            #Looping through the json file\n",
    "            for tags in bot[\"intents\"]:\n",
    "                if tags[\"tag\"] == tag:\n",
    "                    responses = tags[\"responses\"]\n",
    "            print(np.random.choice(responses))\n",
    "        \n",
    "        else:\n",
    "            print(\"I dont quite Understand, Ask another question\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interaction with the ChatBot Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Talking with the Bot,To Stop type quit\n",
      "You :   Hi there\n",
      "Hello!\n",
      "You :   what is your name\n",
      "You can call me Emmy\n",
      "You :   how old are you\n",
      "Less than a Year\n",
      "You :   am a beginner in data science what can i start with\n",
      "Learn Python\n",
      "You :   which course can you recommend\n",
      "Udacity or Cousera\n",
      "You :   after python, resources to learn data science as a beginner\n",
      "Check out Word Quant University and ML research papers\n",
      "You :   which books do you recommend for data science\n",
      "Hands on Scikit Learn and Tensorflow\n",
      "You :   if i am to take a mathematics course on data science recommend one for me\n",
      "Machine Learning A-Z Udemy\n",
      "You :   mathematics course on data science\n",
      "I dont quite Understand, Ask another question\n",
      "You :   maths for data science\n",
      "Fast.ai Algebra for Coders\n",
      "You :   data science blogs and website\n",
      "Hugging Face(NLP)\n",
      "You :   free academic resources for data science\n",
      "Stanford Lectures\n",
      "You :   podcast for data science beginners\n",
      "Data Skeptic\n",
      "You :   what are your operating hour\n",
      "Am still Under Development\n",
      "You :   thanks alot\n",
      "Good to see you again!\n",
      "You :   quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the Chat above we can see that there are some question the chatbot was asked that he could not give answer to it is because we set a threshold to it, if the model predict a sentence that is a below the threshold it prints out the else statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusionâ€¦\n",
    "\n",
    "Building a ChatBot is not Rocket Science for a non developer, since there are toons of website that uses drag and drop software, Intents based app like  [wit.ai](https://www.wit.ai), [open.ai](https://www.open.ai) you can Build a chatbot for yourself and integrate it to your website, applications,or other social media platform to help keeps your conversation on the go even if you are not online.\n",
    "\n",
    "You can fork out this repo and work on it to suit your own need things you need to do is edit the Json File to how best you want your chatbot to interact with the user.\n",
    "\n",
    "As your Responses, Tag, Intents increase you would also have to increase the the layers in your model to have a better accuracy and responses for the chatbot.\n",
    "\n",
    "You can read about about the __Nesterov Accelerated Gradient Descent__\n",
    "\n",
    "__Natural Language Processing__ is an exciting Field that is fast growing you can also try other Deep Learning Framework like Fast.ai, PyTorch or Theano to see how your data learn and how the chatbot works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect with me on [twitter](https://www.twitter.com/imonemmanuel).\n",
    "\n",
    "#### Connect with me on [linkedin](https://www.linkedin.com/in/imonemmanuel)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
